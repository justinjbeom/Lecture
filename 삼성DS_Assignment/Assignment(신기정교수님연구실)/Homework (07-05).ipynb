{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "\n",
    "import metric_learn\n",
    "from metric_learn import MMC_Supervised, MMC, NCA, LMNN\n",
    "\n",
    "import sklearn\n",
    "from sklearn import decomposition, manifold, cluster, datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, MDS\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs, load_digits, make_moons, fetch_olivetti_faces\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Clustering\n",
    "# The purpose of this exercise is to apply basic clustering algorithms covered in class\n",
    "\n",
    "# generate some random cluster data\n",
    "X, y = make_blobs(random_state=100, n_samples=900, centers = 6)\n",
    "rng = np.random.RandomState(74)\n",
    "# visualization of the data\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Clustering (cont)\n",
    "\n",
    "# Let's see if common clustering algorithms could work in this case\n",
    "\n",
    "# KMEANS- Clustering\n",
    "### YOUR CODE HERE (Fill in the 'None'). 2 lines of code. 2 points\n",
    "# Hint: make sure that variable \"labels\" is the array of cluster indices of samples in X\n",
    "kmeans = KMeans(None)\n",
    "labels = None\n",
    "### END OF YOUR CODE\n",
    "\n",
    "# Visualization of the clustered data, result of KMeans\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\"plasma\")\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Clustering (cont)\n",
    "\n",
    "# Agglomerative Clustering\n",
    "### YOUR CODE HERE (Fill in the \"None\"). 2 lines of code. 2 points\n",
    "# Hint: make sure that variable \"y_pred\" is the array of cluster indices of samples in X\n",
    "# Hint: we use euclidean affinity, average linkage\n",
    "agglomerative = AgglomerativeClustering(None)\n",
    "y_pred = None\n",
    "### END OF YOUR CODE.\n",
    "\n",
    "# Visualization of the clustered data, result of Agglomerative Clustering\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=\"viridis\")\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Visualization of some sample images from this dataset\\nfig, ax = plt.subplots(4, 8, subplot_kw=dict(xticks=[], yticks=[]))\\nfor i, axi in enumerate(ax.flat):\\n    axi.imshow(faces.images[i], cmap='gray')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2: Dimensionality Reduction\n",
    "# In the class on 07-05, we saw that Isomap could learn the 2 features of image data\n",
    "# The purpose of this exercise is to examine Isomap on another dataset\n",
    "\n",
    "# Load the face images (it may take several images)\n",
    "faces = fetch_olivetti_faces()\n",
    "\n",
    "# Visualization of some sample images from this dataset\n",
    "fig, ax = plt.subplots(4, 8, subplot_kw=dict(xticks=[], yticks=[]))\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(faces.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Plot the embedded images\\nfig, ax = plt.subplots(figsize=(10, 10))\\nimages = faces.images[:,::2,::2]\\nthumb_frac = 0.05\\ncmap = 'gray'\\nax = ax or plt.gca()\\n    \\nax.plot(proj[:, 0], proj[:, 1], '.k')\\n\\nif images is not None:\\n    min_dist_2 = (thumb_frac * max(proj.max(0) - proj.min(0))) ** 2\\n    shown_images = np.array([2 * proj.max(0)])\\n    for i in range(X.shape[0]):\\n        dist = np.sum((proj[i] - shown_images) ** 2, 1)\\n        if np.min(dist) < min_dist_2:\\n            # don't show points that are too close\\n            continue\\n        shown_images = np.vstack([shown_images, proj[i]])\\n        imagebox = offsetbox.AnnotationBbox(\\n            offsetbox.OffsetImage(images[i], cmap=cmap),\\n                                      proj[i])\\n        ax.add_artist(imagebox)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2: Dimensionality Reduction (cont)\n",
    "\n",
    "# Load the face images\n",
    "X = faces.data\n",
    "\n",
    "# Isomap for X \n",
    "### YOUR CODE HERE (Fill in the \"None\"). 2 lines of code. 2 points.\n",
    "# Hint: repeat the process in the 07-05 class, in which we can map the images to a 2-feature space\n",
    "# Hint: the \"proj\" variable must be the transformed data\n",
    "model = Isomap(None)\n",
    "proj = None\n",
    "### END OF YOUR CODE\n",
    "\n",
    "# Plot the embedded images\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "images = faces.images[:,::2,::2]\n",
    "thumb_frac = 0.05\n",
    "cmap = 'gray'\n",
    "ax = ax or plt.gca()\n",
    "    \n",
    "ax.plot(proj[:, 0], proj[:, 1], '.k')\n",
    "\n",
    "if images is not None:\n",
    "    min_dist_2 = (thumb_frac * max(proj.max(0) - proj.min(0))) ** 2\n",
    "    shown_images = np.array([2 * proj.max(0)])\n",
    "    for i in range(X.shape[0]):\n",
    "        dist = np.sum((proj[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < min_dist_2:\n",
    "            # don't show points that are too close\n",
    "            continue\n",
    "        shown_images = np.vstack([shown_images, proj[i]])\n",
    "        imagebox = offsetbox.AnnotationBbox(\n",
    "            offsetbox.OffsetImage(images[i], cmap=cmap),\n",
    "                                      proj[i])\n",
    "        ax.add_artist(imagebox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Dimensionality Reduction (cont)\n",
    "\n",
    "# Question: based on this mapping, what can we see about the features learned by Isomap? (1 point)\n",
    "# Please pay attention to how images change along the HORIZONTAL axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Metric Learning\n",
    "# The purpose of this exercise is to see effects of Metric learning on data\n",
    "\n",
    "# generate some random cluster data\n",
    "X, y = make_blobs(random_state=100, n_samples=1000, centers = 6)\n",
    "rng = np.random.RandomState(50)\n",
    "# transform the data to be stretched\n",
    "transformation = rng.normal(size=(2, 2))\n",
    "X = np.dot(X, transformation)\n",
    "# visualization of the data\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Metric Learning (cont)\n",
    "\n",
    "# Agglomerative Clustering\n",
    "### YOUR CODE HERE. Fill in the (\"None\"). 2 lines of code. 2 points.\n",
    "# Hint: make sure that variable \"y_pred\" is the array of cluster indices of samples in X\n",
    "# Hint: we use euclidean affinity, average linkage\n",
    "agglomerative = AgglomerativeClustering(None) \n",
    "y_pred = None\n",
    "### END OF YOUR CODE\n",
    "\n",
    "# LMNN metric learning\n",
    "### YOUR CODE HERE (Fill in the \"None\"). 3 lines of code. 2 points\n",
    "# Hint: define our LMNN object, use learning rate \"1e-5\", maximum 100 iterations\n",
    "lmnn = None\n",
    "# Hint: fit the LMNN object with data, AND use labels learned by Agglomerative Clustering learned above\n",
    "None\n",
    "# Hint: perform transformation on the data. \n",
    "X_transformed = None\n",
    "### END OF YOUR CODE\n",
    "\n",
    "# Visualize the effects of LMNN on the transformed data\n",
    "plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c = y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inquiries: manh.it97@kaist.ac.kr (MANH TUAN DO)\n",
    "# Good luck! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
